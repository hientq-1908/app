{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AdamW,    get_linear_schedule_with_warmup, AutoTokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem_df = pd.read_csv('poems.csv')\n",
    "poem_df = poem_df.fillna('')\n",
    "random_seed = 1\n",
    "batch_size= 2\n",
    "epochs=8\n",
    "max_length = 100\n",
    "data_df = poem_df.stanza_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "special_tokens_dict = {\n",
    "    'bos_token': '<BOS>', \n",
    "    'eos_token': '<EOS>', \n",
    "    'pad_token': '<PAD>',\n",
    "    'cls_token': '<CLS>',\n",
    "    'sep_token': '<SEP>',\n",
    "    }\n",
    "num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "num_added_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import PoemDataset\n",
    "poem_dataset = PoemDataset(\n",
    "    data_df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import train_val_split\n",
    "train_set, val_set = train_val_split(split=0.8, dataset=poem_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.manual_seed_all(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_set,\n",
    "    sampler=RandomSampler(train_set),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_set,\n",
    "    sampler=SequentialSampler(val_set),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "eps = 1e-8\n",
    "warmup_steps = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "promt = \"<BOS>\"\n",
    "generated = torch.tensor(tokenizer.encode(promt)).unsqueeze(0)\n",
    "generated = generated.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50262, 768)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration = GPT2Config(vocab_size=len(tokenizer), n_positions=max_length).from_pretrained('gpt2', output_hidden_states=True)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, config=configuration)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=eps)\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps = warmup_steps,\n",
    "    num_training_steps = total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 of 100\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 8.00 GiB total capacity; 6.62 GiB already allocated; 0 bytes free; 6.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tqhie\\OneDrive\\Documents\\deep_learning\\nlp\\poem_generator\\test.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tqhie/OneDrive/Documents/deep_learning/nlp/poem_generator/test.ipynb#ch0000015?line=9'>10</a>\u001b[0m input_ids \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tqhie/OneDrive/Documents/deep_learning/nlp/poem_generator/test.ipynb#ch0000015?line=10'>11</a>\u001b[0m attn_masks \u001b[39m=\u001b[39m attn_masks\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tqhie/OneDrive/Documents/deep_learning/nlp/poem_generator/test.ipynb#ch0000015?line=12'>13</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tqhie/OneDrive/Documents/deep_learning/nlp/poem_generator/test.ipynb#ch0000015?line=13'>14</a>\u001b[0m     input_ids,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tqhie/OneDrive/Documents/deep_learning/nlp/poem_generator/test.ipynb#ch0000015?line=14'>15</a>\u001b[0m     labels\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tqhie/OneDrive/Documents/deep_learning/nlp/poem_generator/test.ipynb#ch0000015?line=15'>16</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattn_masks,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tqhie/OneDrive/Documents/deep_learning/nlp/poem_generator/test.ipynb#ch0000015?line=16'>17</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tqhie/OneDrive/Documents/deep_learning/nlp/poem_generator/test.ipynb#ch0000015?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tqhie/OneDrive/Documents/deep_learning/nlp/poem_generator/test.ipynb#ch0000015?line=19'>20</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1044\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1035'>1036</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1036'>1037</a>\u001b[0m \u001b[39mlabels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1037'>1038</a>\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1038'>1039</a>\u001b[0m \u001b[39m    ``labels = input_ids`` Indices are selected in ``[-100, 0, ..., config.vocab_size]`` All labels set to\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1039'>1040</a>\u001b[0m \u001b[39m    ``-100`` are ignored (masked), the loss is only computed for labels in ``[0, ..., config.vocab_size]``\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1040'>1041</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1041'>1042</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1043'>1044</a>\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1044'>1045</a>\u001b[0m     input_ids,\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1045'>1046</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1046'>1047</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1047'>1048</a>\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1048'>1049</a>\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1049'>1050</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1050'>1051</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1051'>1052</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1052'>1053</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1053'>1054</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1054'>1055</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1055'>1056</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1056'>1057</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1057'>1058</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1058'>1059</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=1060'>1061</a>\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:887\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=876'>877</a>\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=877'>878</a>\u001b[0m         create_custom_forward(block),\n\u001b[0;32m    <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=878'>879</a>\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=883'>884</a>\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=884'>885</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=885'>886</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=886'>887</a>\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[0;32m    <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=887'>888</a>\u001b[0m         hidden_states,\n\u001b[0;32m    <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=888'>889</a>\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[0;32m    <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=889'>890</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=890'>891</a>\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[0;32m    <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=891'>892</a>\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=892'>893</a>\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=893'>894</a>\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=894'>895</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=895'>896</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=897'>898</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=898'>899</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:432\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=429'>430</a>\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m    <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=430'>431</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(hidden_states)\n\u001b[1;32m--> <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=431'>432</a>\u001b[0m feed_forward_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states)\n\u001b[0;32m    <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=432'>433</a>\u001b[0m \u001b[39m# residual connection\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=433'>434</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:359\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=357'>358</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[1;32m--> <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=358'>359</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc_fc(hidden_states)\n\u001b[0;32m    <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=359'>360</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(hidden_states)\n\u001b[0;32m    <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=360'>361</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(hidden_states)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\modeling_utils.py:1762\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/modeling_utils.py?line=1759'>1760</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/modeling_utils.py?line=1760'>1761</a>\u001b[0m     size_out \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnf,)\n\u001b[1;32m-> <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/modeling_utils.py?line=1761'>1762</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49maddmm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, x\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, x\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight)\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/modeling_utils.py?line=1762'>1763</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(\u001b[39m*\u001b[39msize_out)\n\u001b[0;32m   <a href='file:///c%3A/Users/tqhie/anaconda3/envs/myenv/lib/site-packages/transformers/modeling_utils.py?line=1763'>1764</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 8.00 GiB total capacity; 6.62 GiB already allocated; 0 bytes free; 6.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "epochs = 100\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f'epoch {epoch+1} of {epochs}')\n",
    "    epoch_loss = list()\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids, attn_masks = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attn_masks = attn_masks.to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids,\n",
    "            labels=input_ids,\n",
    "            attention_mask=attn_masks,\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        epoch_loss.append(epoch_loss)\n",
    "\n",
    "    avg_loss = np.mean(epoch_loss)\n",
    "    print(f'loss {avg_loss}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f276faaabf584783af755d1ae067aeed7dd8a1944d29073563c0156e1c21bce4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
